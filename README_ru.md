## Генерация песен

Модели генерации песен включают в себя различные виды:
- Генерация музыки (Meta/MaGNET)
- Генерация инструментальной музыки (модель MusicGen).
- Генерация микса музыки+вокала (модель Jukebox).
- Генерация вокала/подписи (модель Барка).
- Генерация звука (аудио-эффектов) (модель AudioGen).
- Генерация песни (текст-текст+музыка+вокал) (Suno.Ai).
- Песня композиции поколения (модели семейства Muzic/Microsoft)

Подходы SOTA, основаны на моделях Трансформера (JukeBox,AudioLM,MusicGen) и моделях Диффузии (Noise2Music). С задачей генерации речи и `пения` хорошо справляется модель Bark, которая основана также на Иерархическом моделлировании (Семантическое, Грубое, Точное).

### Как улучшить качество генерации?

1. С помощью LLM генерировать текстовые подсказки. Каждая подсказка состоит из более точных характеристик музыкальных партий. Его можно сделать более подробным, указав инструмент, темп, жанр или эмоции.
 - преобразование текста в подсказку (с инструментами, жанрами, разделением композиции)
 - преобразование "текста" в "текст-песни" (может быть частью общей подсказки).
В этом случае мы можем генерировать музыку по принципу «подсказка»[+текст]-к-музыке, композиционно. Нам нужно придумать, есть ли необходимость раздельной генерации фоновой музыки и пения (что создает сложности склеивания).

В песне есть партии, поэтому модель должна быть обучена генерировать музыку с пониманием партии/позиции в песне [Position]:
* Введение/введение;
* Стих;
* Прехорус/мост;
* Припев;
* Пост-хор/тег;
* Проигрышный перерыв;
* Конец/утро.

2. Улучшить подход к обучению.
- тренировать на (текст, музыка[-пение]) и на (текст, пение[-музыка]).
- необходимо изучить затраты на разделение аудио для больших наборов данных для создания набора обучающих данных. (например через Demux).
- мы можем использовать CLAP для маркировки обучающего аудио. Ему можно использовать предсказать наиболее релевантный фрагмент текста по аудиофайлу без непосредственной оптимизации для выполнения задачи.

3. Улучшения архитектуры:
 - использовать модели без авторегрессии (например, в модели MagNET), которые показывают 7-10 высокую производительность вывода.
 - использовать LoRA в модели Transformer, сделайте обучение быстрее и дешевле. (необх.изучить)
 - использовать подходы модели диффузии + подход модели трансформатора. (необх.изучить)

## Как работать с репозиторием

* Установите необходимые компоненты с помощью команды «pip install -r requirements.txt».
* Пример обучения модели иерархического преобразователя музыкального генератора, представленный в «example-train-tr-model.ipynb».
* Пример музыки для генерации с моделями HuggingFace, доступными в «hf-music-gen-models».
* Пример сгенерированных образцов, помещенных в папку «generated-samples».

### Architecture/Method Each Models
- Модель MusicGen, основанная на https://arxiv.org/pdf/2306.05284:
  - Архитектура:
    - Аудио Токенизация:
      - Преобразует аудио в квантованные токены с использованием [RVQ](https://arxiv.org/abs/2107.03312) (EnCodec).
      - EnCodec: Сверточный автоэнкодер с латентным пространством, квантованным с помощью остаточной векторной квантизации (RVQ) и потери восстановления с использованием состязательной сети. Токенизатор EnCodec с 4 кодовыми книгами, выборка с частотой 50 Гц.
    - Четыре кодовые книги:
      - Кодовая книга 1: Захватывает высокоуровневую структуру и общие аспекты аудио.
      - Кодовая книга 2: Сосредоточена на промежуточных характеристиках, уточняя детали, предоставленные первой кодовой книгой.
      - Кодовая книга 3: Предоставляет дополнительные детали, работая над нюансами, не захваченными предыдущими кодовыми книгами.
      - Кодовая книга 4: Добавляет финальный слой деталей, обеспечивая высокое качество аудио.
    - Одноуровневая авторегрессионная модель трансформера, обученная на частоте 32 кГц.
    - Декодер трансформера: Авторегрессионная модель, основанная на тексте или мелодии.
  - Обучение:
    - Использовать 20 тысяч часов лицензированной музыки для обучения MusicGen.
    - Внутренний набор данных из 10 тысяч высококачественных музыкальных треков.
    - На данных музыки ShutterStock и Pond5.
  - Страница с примерами: https://ai.honu.io/papers/musicgen/
- Модель Bark (suno.ai): https://github.com/suno-ai/bark
  - Для синтеза текста в речь мы используем модель Bark (Suno, 2023), которая может генерировать реалистичную речь и способна соответствовать тону, высоте, эмоциям и просодии заданного голосового пресета.
  - Архитектура:
    - три модели Трансформера: грубая, текстовая, точная. Тот же подход иерархического моделирования, описанный в [AudioML](https://arxiv.org/pdf/2209.03143).
      - Каждая модель Трансформера основана на трансформере nanoGPT.
- Модель JukeBox: (https://jukebox.openai.com/):
  - Архитектура:
    - Аудио Токенизация:
      - Для сжатия аудио до низкоразмерного пространства используются 3 отдельных иерархических модели [VQ-VAE](https://arxiv.org/pdf/1711.00937).
        - Три каскадные модели.
    - авторегрессивные разреженные Трансформеры.
    - авторегрессивные апсемплеры для восстановления утраченной информации на каждом уровне сжатия.
  - Обучение:
    - Для музыкального VQ-VAE мы используем 3 уровня узких мест, сжимая аудио 44 кГц в размерности на 8x, 32x и 128x.
    - Размер кодовых книг 2048 для каждого уровня.
    - VQ-VAE имеет 2 миллиона параметров и обучается на аудиоклипах длительностью 9 секунд на 256 V100 в течение 3 дней.

## References

### General models
Papers: ComputerScience.Sound (cs.SD) https://arxiv.org/list/cs.SD/recent

## Transformer pased generation:
(MagNET) Masked Audio Generation using a Single Non-Autoregressive Transformer
- Paper: https://arxiv.org/abs/2401.04577
- ModelCard: https://huggingface.co/models?other=magnet
- Samples: https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT/

Jukebox: A Generative Model for Music
- Paper: https://arxiv.org/pdf/2005.00341 (2020/OpenAI)
- Samples: https://jukebox.openai.com/?song=804331648

Simple and Controllable Music Generation (MusicGen)
- Paper: https://arxiv.org/pdf/2306.05284 (2024/MetaAI)
- Samples: https://audiocraft.metademolab.com/musicgen.html

MusicLM: Generating Music From Text
- Paper: https://arxiv.org/pdf/2301.11325 (2023/Google)
- Samples: https://google-research.github.io/seanet/musiclm/examples/

Music Transformer
- Paper: https://arxiv.org/pdf/1809.04281 (2018/Google Brain)

## Diffusion based generation:

Noise2Music: Text-conditioned Music Generation with Diffusion Models
- Paper: https://arxiv.org/abs/2302.03917 (2023)

### Vocal & Singing

Bark/SunoAI (Transformer based):
- Source: https://github.com/suno-ai/bark

RapVerse (scaling autoregressive multimodal transformers): Coherent Vocals and Whole-Body Motions Generations from Text
https://arxiv.org/pdf/2405.20336

### Audio Neural Codecs
High Fidelity Neural Audio Compression (EnCodec) - https://arxiv.org/pdf/2210.13438

### Convoluntional Models Audio Generation
WaveNet: A Generative Model for Raw Audio - https://arxiv.org/pdf/1609.03499

### Compositional Audio
WavJourney: Compositional Audio Creation with Large Language Models - https://arxiv.org/pdf/2307.14335