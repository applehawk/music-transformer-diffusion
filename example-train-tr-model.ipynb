{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is example, how to train Text-to-Music model from Scratch (based on musiclm_pytorch, which include )\n",
    "\n",
    "This is top-level approach, without implementation details of quantizers (VQ-VAE, or RVQ) and Transformer models. A parametric library of models is used that can be customized for the tasks of studying musical fragments and generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First stage: Training MuLaN model (https://arxiv.org/pdf/2208.12415) \n",
    "\n",
    "![](https://github.com/MaxMax2016/musiclm-pytorch/blob/main/musiclm.png?raw=true)\n",
    "\n",
    "MuLan: new generation of acoustic models that link music audio directly to un-constrained natural language music descriptions.\n",
    "It helps us generate (audio, text) pairs, based only on audio files dataset, so we don't need labeled them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from musiclm_pytorch import MuLaN, AudioSpectrogramTransformer, TextTransformer\n",
    "\n",
    "audio_transformer = AudioSpectrogramTransformer(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    dim_head = 64,\n",
    "    spec_n_fft = 128,\n",
    "    spec_win_length = 24,\n",
    "    spec_aug_stretch_factor = 0.8\n",
    ")\n",
    "\n",
    "text_transformer = TextTransformer(\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    dim_head = 64\n",
    ")\n",
    "\n",
    "# based on https://arxiv.org/pdf/2208.12415\n",
    "mulan = MuLaN(\n",
    "    audio_transformer = audio_transformer,\n",
    "    text_transformer = text_transformer\n",
    ")\n",
    "\n",
    "# get a ton of <sound, text> pairs and train\n",
    "\n",
    "wavs = torch.randn(2, 1024)\n",
    "texts = torch.randint(0, 20000, (2, 256))\n",
    "\n",
    "loss = mulan(wavs, texts)\n",
    "loss.backward()\n",
    "\n",
    "# after much training, you can embed sounds and text into a joint embedding space\n",
    "# for conditioning the audio LM\n",
    "embeds = mulan.get_audio_latents(wavs)  # during training\n",
    "embeds = mulan.get_text_latents(texts)  # during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from musiclm_pytorch import MuLaNEmbedQuantizer\n",
    "\n",
    "# setup the quantizer with the namespaced conditioning embeddings, unique per quantizer as well as namespace (per transformer)\n",
    "\n",
    "quantizer = MuLaNEmbedQuantizer(\n",
    "    mulan = mulan,                          # pass in trained mulan from above\n",
    "    conditioning_dims = (1024, 1024, 1024), # say all three transformers have model dimensions of 1024\n",
    "    namespaces = ('semantic', 'coarse', 'fine'),\n",
    ")\n",
    "\n",
    "# now say you want the conditioning embeddings for semantic transformer\n",
    "\n",
    "wavs = torch.randn(2, 1024)\n",
    "conds = quantizer(wavs = wavs, namespace = 'semantic') # (2, 8, 1024) - 8 is number of quantizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical modeling of semantic and acoustic tokens\n",
    "\n",
    "![](./img/audiolm-hierarchy-modeling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Semantic tokens that allow the modeling of long-term structure, extracted from models pretrained on audio data with the objective of masked lan- guage modeling\n",
    "\n",
    "Semantic modeling. The first stage models p(zt |z<t ), the\n",
    "autoregressive prediction of semantic tokens to capture long-term temporal structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from audiolm_pytorch import HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer\n",
    "\n",
    "torch.set_default_device(\"mps\")\n",
    "\n",
    "wav2vec = HubertWithKmeans(\n",
    "    checkpoint_path = './models/hubert/hubert_base_ls960.pt',\n",
    "    kmeans_path = './models/hubert/hubert_base_ls960_L9_km500.bin' \n",
    ")\n",
    "\n",
    "semantic_transformer = SemanticTransformer(\n",
    "    num_semantic_tokens = wav2vec.codebook_size,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    audio_text_condition = True      # this must be set to True (same for CoarseTransformer and FineTransformers)\n",
    ")\n",
    "\n",
    "trainer = SemanticTransformerTrainer(\n",
    "    transformer = semantic_transformer,\n",
    "    wav2vec = wav2vec,\n",
    "    audio_conditioner = quantizer,   # pass in the MulanEmbedQuantizer instance above\n",
    "    folder = './dataset/music_data',\n",
    "    batch_size = 1,\n",
    "    data_max_length = 320 * 32,\n",
    "    num_train_steps = 1\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Coarse acoustic modeling conditioned on the semantic tokens. Acoustic tokens, provided by a neural audio codec, for capturing fine acoustic details. This allows AudioLM to generate coherent and high-quality speech as well as piano music continuations without relying on tran- scripts or symbolic music representations.\n",
    "\n",
    "`Coarse acoustic modeling. The second stage proceeds analogously on the acoustic tokens, but it only predicts the acoustic tokens from the coarse Q′ SoundStream quantizers, conditioned on the semantic tokens. Due to residual quantization in SoundStream, the acoustic tokens have a hierarchical structure: tokens from the coarse quantizers recover acoustic properties like speaker identity and recording conditions, while leaving only the fine acoustic details to the fine quantizer tokens, which are modeled by the next stage. We rely on the simple approach of flattening the acoustic tokens in a row-major order to handle their hierarchical structure.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from unittest import mock\n",
    "from audiolm_pytorch import SoundStream, CoarseTransformer, CoarseTransformerTrainer\n",
    "from audiolm_pytorch import AudioLMSoundStream, MusicLMSoundStream\n",
    "\n",
    "# soundstream = SoundStream.init_and_load_from('/path/to/trained/soundstream.pt')\n",
    "soundstream = MusicLMSoundStream() \n",
    "\n",
    "coarse_transformer = CoarseTransformer(\n",
    "    num_semantic_tokens = wav2vec.codebook_size,\n",
    "    codebook_size = 1024,\n",
    "    num_coarse_quantizers = 4,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    audio_text_condition = True\n",
    ")\n",
    "\n",
    "with mock.patch('builtins.input', return_value='n'):\n",
    "    trainer = CoarseTransformerTrainer(\n",
    "        transformer = coarse_transformer,\n",
    "        codec = soundstream,\n",
    "        wav2vec = wav2vec,\n",
    "        audio_conditioner = quantizer,\n",
    "        folder = './dataset/music_data',\n",
    "        batch_size = 1,\n",
    "        data_max_length = 320 * 32,\n",
    "        num_train_steps = 1\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) fine acoustic modeling\n",
    "\n",
    "`The third stage operates on acoustic tokens corresponding to the fine quantizers, using the Q′ coarse tokens as conditioning and modeling the conditional probability distribution p(yq|y≤Q′ , y>Q′ , y<q) for q > Q′.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from audiolm_pytorch import SoundStream, FineTransformer, FineTransformerTrainer\n",
    "from audiolm_pytorch import AudioLMSoundStream, MusicLMSoundStream\n",
    "\n",
    "fine_transformer = FineTransformer(\n",
    "    num_coarse_quantizers = 4,\n",
    "    num_fine_quantizers = 8,\n",
    "    codebook_size = 1024,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    audio_text_condition = True\n",
    ")\n",
    "\n",
    "with mock.patch('builtins.input', return_value='n'):\n",
    "    trainer = FineTransformerTrainer(\n",
    "        transformer = fine_transformer,\n",
    "        codec = soundstream,\n",
    "        folder = './dataset/music_data',\n",
    "        batch_size = 1,\n",
    "        data_max_length = 320 * 32,\n",
    "        num_train_steps = 1,\n",
    "        audio_conditioner = quantizer\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On this step we trained quantizer and 3 Transformers (Semantic, Coarse, Fine), and ready generation Audio/Music. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiolm_pytorch import AudioLM\n",
    "from musiclm_pytorch import MusicLM\n",
    "\n",
    "audiolm = AudioLM(\n",
    "    wav2vec = wav2vec,\n",
    "    codec = soundstream,\n",
    "    semantic_transformer = semantic_transformer,\n",
    "    coarse_transformer = coarse_transformer,\n",
    "    fine_transformer = fine_transformer\n",
    ")\n",
    "\n",
    "musiclm = MusicLM(\n",
    "    audio_lm = audiolm,\n",
    "    mulan_embed_quantizer = quantizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music = musiclm('the crystalline sounds of the piano in a ballroom', num_samples = 1) # sample 4 and pick the top match with mulan\n",
    "torch.save(music, 'generated_music.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"out.wav\"\n",
    "sample_rate = 44100\n",
    "torchaudio.save(output_path, music.cpu(), sample_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
