{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82ca1a23",
   "metadata": {},
   "source": [
    "<img src=\"./img/jukebox.svg\" width=\"450\">\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/model_doc/jukebox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffa6ed7a-951b-4d59-8592-f4425f463057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:45<00:00, 11.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import JukeboxTokenizer, JukeboxModel\n",
    "\n",
    "tokenizer = JukeboxTokenizer.from_pretrained(\"openai/jukebox-5b-lyrics\")\n",
    "model = JukeboxModel.from_pretrained(\"openai/jukebox-5b-lyrics\")\n",
    "\n",
    "lyrics = \"Hey, are you awake? Can you talk to me?\"\n",
    "artist = \"Zac Brown Band\"\n",
    "genre = \"Country\"\n",
    "metas = tokenizer(artist=artist, genres=genre, lyrics=lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cae58578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Range is [1049580.0,26460000.0), got tensor([[400.]])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m music_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mancestral_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/dl/music-lm-train/.env/lib/python3.12/site-packages/transformers/models/jukebox/modeling_jukebox.py:2610\u001b[0m, in \u001b[0;36mJukeboxModel.ancestral_sample\u001b[0;34m(self, labels, n_samples, **sampling_kwargs)\u001b[0m\n\u001b[1;32m   2606\u001b[0m sample_levels \u001b[38;5;241m=\u001b[39m sampling_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_levels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriors))))\n\u001b[1;32m   2607\u001b[0m music_tokens \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   2608\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros(n_samples, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mlabels[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriors))\n\u001b[1;32m   2609\u001b[0m ]\n\u001b[0;32m-> 2610\u001b[0m music_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmusic_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_levels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msampling_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2611\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m music_tokens\n",
      "File \u001b[0;32m~/Code/dl/music-lm-train/.env/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/dl/music-lm-train/.env/lib/python3.12/site-packages/transformers/models/jukebox/modeling_jukebox.py:2537\u001b[0m, in \u001b[0;36mJukeboxModel._sample\u001b[0;34m(self, music_tokens, labels, sample_levels, metas, chunk_size, sampling_temperature, lower_batch_size, max_batch_size, sample_length_in_seconds, compute_alignments, sample_tokens, offset, save_results, sample_length)\u001b[0m\n\u001b[1;32m   2535\u001b[0m hop_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhop_fraction[level] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriors[level]\u001b[38;5;241m.\u001b[39mn_ctx)\n\u001b[1;32m   2536\u001b[0m max_batch_size \u001b[38;5;241m=\u001b[39m lower_batch_size \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;241m!=\u001b[39m sample_levels \u001b[38;5;28;01melse\u001b[39;00m max_batch_size\n\u001b[0;32m-> 2537\u001b[0m music_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_level\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmusic_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2540\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2541\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_token_to_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_results:\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvqvae\u001b[38;5;241m.\u001b[39mto(music_tokens[level]\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Code/dl/music-lm-train/.env/lib/python3.12/site-packages/transformers/models/jukebox/modeling_jukebox.py:2423\u001b[0m, in \u001b[0;36mJukeboxModel.sample_level\u001b[0;34m(self, music_tokens, labels, offset, sampling_kwargs, level, total_length, hop_length, max_batch_size)\u001b[0m\n\u001b[1;32m   2418\u001b[0m         music_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_single_window(\n\u001b[1;32m   2419\u001b[0m             music_tokens, labels, offset, sampling_kwargs, level, start, max_batch_size\n\u001b[1;32m   2420\u001b[0m         )\n\u001b[1;32m   2422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2423\u001b[0m     music_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_partial_window\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmusic_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batch_size\u001b[49m\n\u001b[1;32m   2425\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m music_tokens\n",
      "File \u001b[0;32m~/Code/dl/music-lm-train/.env/lib/python3.12/site-packages/transformers/models/jukebox/modeling_jukebox.py:2350\u001b[0m, in \u001b[0;36mJukeboxModel.sample_partial_window\u001b[0;34m(self, music_tokens, labels, offset, sampling_kwargs, level, tokens_to_sample, max_batch_size)\u001b[0m\n\u001b[1;32m   2347\u001b[0m     sampling_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m n_ctx\n\u001b[1;32m   2348\u001b[0m     start \u001b[38;5;241m=\u001b[39m nb_sampled_tokens \u001b[38;5;241m-\u001b[39m n_ctx \u001b[38;5;241m+\u001b[39m tokens_to_sample\n\u001b[0;32m-> 2350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_single_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmusic_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/dl/music-lm-train/.env/lib/python3.12/site-packages/transformers/models/jukebox/modeling_jukebox.py:2396\u001b[0m, in \u001b[0;36mJukeboxModel.sample_single_window\u001b[0;34m(self, music_tokens, labels, offset, sampling_kwargs, level, start, max_batch_size)\u001b[0m\n\u001b[1;32m   2390\u001b[0m     name \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAncestral\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrimed\u001b[39m\u001b[38;5;124m\"\u001b[39m][music_tokens_i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2391\u001b[0m     iterator\u001b[38;5;241m.\u001b[39mset_description(\n\u001b[1;32m   2392\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[prior level \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Sampling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens out of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2393\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_length\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mprior\u001b[38;5;241m.\u001b[39mraw_to_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2394\u001b[0m         refresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2395\u001b[0m     )\n\u001b[0;32m-> 2396\u001b[0m     tokens_i \u001b[38;5;241m=\u001b[39m \u001b[43mprior\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmusic_tokens_i\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmusic_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmusic_tokens_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmusic_tokens_conds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmusic_tokens_conds_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2401\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msampling_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2403\u001b[0m     tokens\u001b[38;5;241m.\u001b[39mappend(tokens_i)\n\u001b[1;32m   2404\u001b[0m sampled_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(tokens, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Code/dl/music-lm-train/.env/lib/python3.12/site-packages/transformers/models/jukebox/modeling_jukebox.py:2096\u001b[0m, in \u001b[0;36mJukeboxPrior.sample\u001b[0;34m(self, n_samples, music_tokens, music_tokens_conds, metadata, temp, top_k, top_p, chunk_size, sample_tokens)\u001b[0m\n\u001b[1;32m   2092\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sampling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples with temp=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, top_k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, top_p=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_p\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2094\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m   2095\u001b[0m     \u001b[38;5;66;03m# Currently audio_conditioning only uses immediately above layer\u001b[39;00m\n\u001b[0;32m-> 2096\u001b[0m     audio_conditioning, metadata_conditioning, lyric_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cond\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmusic_tokens_conds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2097\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n\u001b[1;32m   2098\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m no_past_context:  \u001b[38;5;66;03m# the prime_sample function will be used with music_tokens set to None\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/dl/music-lm-train/.env/lib/python3.12/site-packages/transformers/models/jukebox/modeling_jukebox.py:2048\u001b[0m, in \u001b[0;36mJukeboxPrior.get_cond\u001b[0;34m(self, music_tokens_conds, metadata)\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2046\u001b[0m     metadata, lyric_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2047\u001b[0m metadata_conditioning, metadata_pos \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 2048\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata_conditioning \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   2049\u001b[0m )\n\u001b[1;32m   2050\u001b[0m audio_conditioning \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens(music_tokens_conds) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_conditioning \u001b[38;5;28;01melse\u001b[39;00m metadata_pos\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m audio_conditioning, metadata_conditioning, lyric_tokens\n",
      "File \u001b[0;32m~/Code/dl/music-lm-train/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/dl/music-lm-train/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/dl/music-lm-train/.env/lib/python3.12/site-packages/transformers/models/jukebox/modeling_jukebox.py:1754\u001b[0m, in \u001b[0;36mJukeboxLabelConditioner.forward\u001b[0;34m(self, metadata)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     start \u001b[38;5;241m=\u001b[39m start\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   1752\u001b[0m     end \u001b[38;5;241m=\u001b[39m end\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   1753\u001b[0m     pos_emb \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_length_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabsolute_pos_emb(start, end)\n\u001b[1;32m   1756\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_pos_emb(start \u001b[38;5;241m/\u001b[39m total_length, end \u001b[38;5;241m/\u001b[39m total_length)\n\u001b[1;32m   1757\u001b[0m     )\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1759\u001b[0m     pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/dl/music-lm-train/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/dl/music-lm-train/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/dl/music-lm-train/.env/lib/python3.12/site-packages/transformers/models/jukebox/modeling_jukebox.py:1682\u001b[0m, in \u001b[0;36mJukeboxRangeEmbedding.forward\u001b[0;34m(self, pos_start, pos_end)\u001b[0m\n\u001b[1;32m   1680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected shape with 2 dims, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos_start\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_min \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m pos_start)\u001b[38;5;241m.\u001b[39mall() \u001b[38;5;129;01mand\u001b[39;00m (pos_start \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_max)\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m-> 1682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange is [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_min\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_max\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos_start\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1684\u001b[0m pos_start \u001b[38;5;241m=\u001b[39m pos_start\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos_end \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Range is [1049580.0,26460000.0), got tensor([[400.]])"
     ]
    }
   ],
   "source": [
    "music_tokens = model.ancestral_sample(metas.input_ids, sample_length=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
