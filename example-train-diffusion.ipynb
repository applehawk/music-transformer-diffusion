{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подходит для генерации с помощью диффузии\n",
    "pip install audio-diffusion-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_diffusion_pytorch import DiffusionModel, UNetV0, VDiffusion, VSampler\n",
    "\n",
    "model = DiffusionModel(\n",
    "    # ... same as unconditional model\n",
    "    use_text_conditioning=True, # U-Net: enables text conditioning (default T5-base)\n",
    "    use_embedding_cfg=True, # U-Net: enables classifier free guidance\n",
    "    embedding_max_length=64, # U-Net: text embedding maximum length (default for T5-base)\n",
    "    embedding_features=768, # U-Net: text mbedding features (default for T5-base)\n",
    "    cross_attentions=[0, 0, 0, 1, 1, 1, 1, 1, 1], # U-Net: cross-attention enabled/disabled at each layer\n",
    ")\n",
    "\n",
    "# Train model with audio waveforms\n",
    "audio_wave = torch.randn(1, 2, 2**18) # [batch, in_channels, length]\n",
    "\n",
    "loss = model(\n",
    "    audio_wave,\n",
    "    text=['The audio description'], # Text conditioning, one element per batch\n",
    "    embedding_mask_proba=0.1 # Probability of masking text with learned embedding (Classifier-Free Guidance Mask)\n",
    ")\n",
    "loss.backward()\n",
    "\n",
    "# Turn noise into new audio sample with diffusion\n",
    "noise = torch.randn(1, 2, 2**18)\n",
    "\n",
    "sample = model.sample(\n",
    "    noise,\n",
    "    text=['The audio description'],\n",
    "    embedding_scale=5.0, # Higher for more text importance, suggested range: 1-15 (Classifier-Free Guidance Scale)\n",
    "    num_steps=2 # Higher for better quality, suggested num_steps: 10-100\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
